# GPU Feeder V3 - Plan de Migration
## Fusion Architecture Serveur HTTP Persistant (v1) + Tracking SQLite (v2)

**Date**: 2026-02-04
**Objectif**: CrÃ©er GPU Feeder v3 en fusionnant les meilleures parties de v1 et v2

---

## ğŸ¯ Architecture Cible V3

### Principe
- **Serveurs vLLM persistants** (v1) lancÃ©s au dÃ©marrage, modÃ¨le chargÃ© 1Ã—
- **RequÃªtes HTTP individuelles** vers OpenAI API `/v1/chat/completions`
- **Continuous batching automatique** gÃ©rÃ© par vLLM (pas de fichiers JSONL)
- **Tracking SQLite** (v2) avec fan-in pour jobs fragmentÃ©s
- **GranularitÃ© 512 tokens** avec chunking et overlap 120 tokens
- **Orchestration intelligente** (v1) avec stratÃ©gie d'allocation dynamique

### Composants Ã  Fusionner

| Composant | Source | Raison |
|-----------|--------|--------|
| `manager.go` | V1 | Lifecycle containers persistants |
| `service.go` | V1 | Orchestration 3 goroutines |
| `allocator.go` | V1 | StratÃ©gie allocation dynamique |
| `monitor.go` | V1 | GPU monitoring nvidia-smi |
| `health.go` | V1 | Health checking continu |
| `schema.sql` | V2 | Table `gpu_jobs` avec fan-in |
| `fanin.go` | V2 | Logique agrÃ©gation fragments |
| `db.go` | V2 | OpÃ©rations atomiques SQLite |

---

## ğŸ“‚ Structure Fichiers V3

```
/inference/horos47/
â”œâ”€â”€ cmd/
â”‚   â””â”€â”€ gpu_feeder_v3/
â”‚       â””â”€â”€ main.go              â† Nouveau: Fusion v1 JSON-RPC + v2 worker
â”œâ”€â”€ services/gpufeeder/
â”‚   â”œâ”€â”€ service.go               â† V1 (orchestration)
â”‚   â”œâ”€â”€ manager.go               â† V1 (lifecycle containers)
â”‚   â”œâ”€â”€ allocator.go             â† V1 (stratÃ©gie allocation)
â”‚   â”œâ”€â”€ monitor.go               â† V1 (GPU monitoring)
â”‚   â”œâ”€â”€ health.go                â† V1 (health checking)
â”‚   â”œâ”€â”€ types.go                 â† V1 (structures communes)
â”‚   â”œâ”€â”€ worker.go                â† NOUVEAU: Worker qui poll SQLite
â”‚   â”œâ”€â”€ db.go                    â† V2 (opÃ©rations SQLite)
â”‚   â”œâ”€â”€ fanin.go                 â† V2 (agrÃ©gation fragments)
â”‚   â”œâ”€â”€ schema.sql               â† V2 (table gpu_jobs)
â”‚   â”œâ”€â”€ http_client.go           â† NOUVEAU: Client HTTP pour vLLM
â”‚   â””â”€â”€ config.go                â† NOUVEAU: Configuration v3
â””â”€â”€ bin/
    â””â”€â”€ gpu_feeder_v3            â† Binaire final
```

---

## ğŸ”§ Modifications Critiques

### 1. Nouveau Fichier: `worker.go`

**RÃ´le**: Poll la table `gpu_jobs` et envoie requÃªtes HTTP vers serveurs vLLM persistants.

```go
package gpufeeder

// Worker poll gpu_jobs et envoie vers vLLM HTTP
type Worker struct {
    db          *sql.DB
    logger      *slog.Logger
    manager     *ProcessManager
    httpClient  *VLLMHTTPClient
    pollInterval time.Duration
}

// Run dÃ©marre boucle de polling
func (w *Worker) Run(ctx context.Context) error {
    ticker := time.NewTicker(w.pollInterval)
    defer ticker.Stop()

    for {
        select {
        case <-ctx.Done():
            return ctx.Err()
        case <-ticker.C:
            w.processBatch(ctx)
        }
    }
}

// processBatch claim batch et envoie requÃªtes HTTP
func (w *Worker) processBatch(ctx context.Context) error {
    // 1. DÃ©cider modÃ¨le (Think prioritaire)
    modelType := w.selectModel(ctx)
    if modelType == "" {
        return nil
    }

    // 2. Claim batch atomique
    jobs, err := w.claimBatch(ctx, modelType, batchSize)
    if err != nil {
        return err
    }

    // 3. VÃ©rifier que serveur vLLM est actif
    serverURL := w.getServerURL(modelType)
    if !w.manager.IsInstanceRunning(modelType) {
        w.logger.Warn("vLLM instance not running", "model", modelType)
        w.releaseBatch(ctx, jobs)
        return nil
    }

    // 4. Envoyer requÃªtes HTTP en parallÃ¨le
    var wg sync.WaitGroup
    for _, job := range jobs {
        wg.Add(1)
        go func(j Job) {
            defer wg.Done()
            w.processJob(ctx, j, serverURL)
        }(job)
    }
    wg.Wait()

    return nil
}

// processJob envoie 1 requÃªte HTTP vers vLLM
func (w *Worker) processJob(ctx context.Context, job Job, serverURL string) error {
    // Parser payload
    payload, err := w.loadPayload(job.PayloadPath)
    if err != nil {
        w.failJob(ctx, job, err)
        return err
    }

    // Construire requÃªte OpenAI
    req := w.buildVLLMRequest(payload, job.ModelType)

    // Envoyer HTTP POST
    resp, err := w.httpClient.SendRequest(ctx, serverURL, req)
    if err != nil {
        w.failJob(ctx, job, err)
        return err
    }

    // Sauver rÃ©sultat
    if err := w.completeJob(ctx, job, resp); err != nil {
        return err
    }

    // Check fan-in
    w.checkFanIn(ctx, job.ID)

    return nil
}
```

### 2. Nouveau Fichier: `http_client.go`

**RÃ´le**: Client HTTP rÃ©utilisable pour communiquer avec serveurs vLLM.

```go
package gpufeeder

import (
    "bytes"
    "context"
    "encoding/json"
    "fmt"
    "net/http"
    "time"
)

// VLLMHTTPClient client HTTP pour vLLM OpenAI API
type VLLMHTTPClient struct {
    client *http.Client
    logger *slog.Logger
}

func NewVLLMHTTPClient(logger *slog.Logger) *VLLMHTTPClient {
    return &VLLMHTTPClient{
        client: &http.Client{
            Timeout: 120 * time.Second,
        },
        logger: logger,
    }
}

// SendRequest envoie requÃªte vers vLLM et retourne rÃ©ponse
func (c *VLLMHTTPClient) SendRequest(ctx context.Context, serverURL string, req VLLMRequest) (*VLLMResponse, error) {
    reqJSON, err := json.Marshal(req)
    if err != nil {
        return nil, fmt.Errorf("marshal request: %w", err)
    }

    httpReq, err := http.NewRequestWithContext(ctx, "POST", serverURL+"/v1/chat/completions", bytes.NewReader(reqJSON))
    if err != nil {
        return nil, err
    }
    httpReq.Header.Set("Content-Type", "application/json")

    c.logger.Debug("Sending vLLM request", "url", serverURL, "size", len(reqJSON))

    resp, err := c.client.Do(httpReq)
    if err != nil {
        return nil, fmt.Errorf("http request: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        body, _ := io.ReadAll(resp.Body)
        return nil, fmt.Errorf("vLLM error %d: %s", resp.StatusCode, string(body))
    }

    var vllmResp VLLMResponse
    if err := json.NewDecoder(resp.Body).Decode(&vllmResp); err != nil {
        return nil, fmt.Errorf("decode response: %w", err)
    }

    return &vllmResp, nil
}
```

### 3. Modification: `service.go`

**Ajouter** mÃ©thode pour que Worker puisse vÃ©rifier si instance est active:

```go
// IsInstanceRunning vÃ©rifie si instance vLLM est active
func (s *Service) IsInstanceRunning(modelType string) bool {
    instanceName := "vllm-vision"
    if modelType == "think" {
        instanceName = "vllm-think"
    }

    _, exists := s.manager.GetInstance(instanceName)
    return exists
}

// GetServerURL retourne URL serveur selon type modÃ¨le
func (s *Service) GetServerURL(modelType string) string {
    if modelType == "think" {
        return "http://localhost:8002"
    }
    return "http://localhost:8001"
}
```

### 4. Modification: `main.go`

**Architecture hybride**: JSON-RPC pour client externe + Worker interne pour SQLite.

```go
package main

import (
    "context"
    "database/sql"
    "encoding/json"
    "io"
    "log/slog"
    "os"
    "os/signal"
    "syscall"

    "horos47/core/data"
    "horos47/services/gpufeeder"
)

func main() {
    logger := setupLogger()
    logger.Info("GPU Feeder V3 starting")

    // 1. Connexion DB principale (workload stats)
    mainDB, err := data.OpenDB("/inference/horos47/data/main.db")
    if err != nil {
        logger.Error("Failed to open main DB", "error", err)
        os.Exit(1)
    }
    defer mainDB.Close()

    // 2. Connexion DB jobs (gpu_jobs table)
    jobsDB, err := sql.Open("sqlite", "/tmp/gpu_feeder_v3/jobs.db")
    if err != nil {
        logger.Error("Failed to open jobs DB", "error", err)
        os.Exit(1)
    }
    defer jobsDB.Close()

    // Init schema
    if err := gpufeeder.InitSchema(jobsDB); err != nil {
        logger.Error("Failed to init schema", "error", err)
        os.Exit(1)
    }

    // 3. CrÃ©er service (orchestration + containers)
    svc := gpufeeder.New(mainDB, logger)

    // 4. CrÃ©er worker (poll gpu_jobs)
    worker := gpufeeder.NewWorker(jobsDB, logger, svc)

    // Context avec signal handling
    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()

    sigChan := make(chan os.Signal, 1)
    signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
    go func() {
        <-sigChan
        logger.Info("Shutdown signal received")
        cancel()
    }()

    // 5. DÃ©marrer service (lance containers)
    if err := svc.Start(ctx); err != nil {
        logger.Error("Failed to start service", "error", err)
        os.Exit(1)
    }

    // 6. DÃ©marrer worker (poll jobs)
    go func() {
        if err := worker.Run(ctx); err != nil {
            logger.Error("Worker failed", "error", err)
        }
    }()

    logger.Info("GPU Feeder V3 ready")
    logger.Info("- Orchestration: monitoring GPU, allocating resources")
    logger.Info("- Worker: polling gpu_jobs table")
    logger.Info("- JSON-RPC: listening on stdin (MCP compatible)")

    // 7. JSON-RPC loop (pour clients externes si nÃ©cessaire)
    runJSONRPCServer(ctx, logger, svc)

    // Shutdown
    if err := svc.Close(); err != nil {
        logger.Error("Error during shutdown", "error", err)
    }
}
```

---

## ğŸ—‚ï¸ GranularitÃ© et Chunking

### StratÃ©gie Chunking 512 Tokens

**OÃ¹ chunker?**
En **amont** lors de la crÃ©ation des jobs dans `handler_image_to_ocr.go`.

**Modification** `/inference/horos47/services/ingest/handler_image_to_ocr.go`:

```go
// Au lieu de crÃ©er 1 job OCR par page, crÃ©er N jobs fragmentÃ©s
func (s *Service) createOCRJobsWithChunking(ctx context.Context, imagePath string, docID string, pageNum int) error {
    // Si image > 4MB, fragmenter en tiles de 512x512 avec overlap 120px
    imageSize := getImageSize(imagePath)

    if imageSize > 4*1024*1024 {
        // CrÃ©er job parent
        parentID := data.NewUUID()

        // DÃ©couper image en tiles
        tiles := splitImageIntoTiles(imagePath, 512, 120)

        // CrÃ©er 1 job GPU par tile
        for idx, tile := range tiles {
            payload := map[string]interface{}{
                "image_path": tile.Path,
                "fragment_index": idx + 1,
                "total_fragments": len(tiles),
            }

            // Insert dans gpu_jobs avec parent_id
            insertGPUJob(ctx, parentID, payload, "vision")
        }
    } else {
        // Image petite: 1 seul job
        payload := map[string]interface{}{
            "image_path": imagePath,
            "fragment_index": 1,
            "total_fragments": 1,
        }
        insertGPUJob(ctx, nil, payload, "vision")
    }
}
```

**Note**: Pour l'instant, garder chunking simple (1 job = 1 image). Optimiser aprÃ¨s validation architecture.

---

## ğŸ§ª Configuration vLLM Optimale

### ParamÃ¨tres Gemini (dÃ©jÃ  dans v1)

```go
// manager.go - LaunchVisionVLLM
args := []string{
    "--model", "/models/qwen2-vl-7b-instruct",
    "--dtype", "bfloat16",
    "--gpu-memory-utilization", "0.75",
    "--max-model-len", "16384",
    "--max-num-seqs", "8",
    "--enable-chunked-prefill",            // Chunked prefill activÃ©
    "--max-num-batched-tokens", "2048",    // NOUVEAU: Optimal throughput
    "--disable-log-requests",              // NOUVEAU: RÃ©duire overhead logs
}
```

**RÃ©fÃ©rences**:
- Default `max_num_batched_tokens=512` pour meilleure latence (ITL)
- Valeur `2048` pour meilleur throughput (TTFT)
- Source: [vLLM Optimization Guide](https://docs.vllm.ai/en/stable/configuration/optimization/)

---

## ğŸ”„ Flux de DonnÃ©es V3

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Handler Image â†’ OCR (ingest service)                       â”‚
â”‚  - CrÃ©e jobs dans gpu_jobs table                           â”‚
â”‚  - 1 job = 1 image (ou N fragments si chunking)            â”‚
â”‚  - parent_id pour fan-in                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ INSERT gpu_jobs
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SQLite: gpu_jobs table                                     â”‚
â”‚  - status='pending', model_type='vision'                    â”‚
â”‚  - payload_sha256 pour idempotence                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ POLL (5s)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker.processBatch()                                      â”‚
â”‚  - SELECT jobs WHERE status='pending' LIMIT 32              â”‚
â”‚  - UPDATE status='processing', batch_id=uuid                â”‚
â”‚  - VÃ©rifier serveur vLLM actif                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ HTTP POST (parallÃ¨le)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Serveur vLLM Vision (persistant, port 8001)               â”‚
â”‚  - Continuous batching automatique                          â”‚
â”‚  - Traite requÃªtes en parallÃ¨le (max_num_seqs=8)          â”‚
â”‚  - Retourne rÃ©ponses OpenAI format                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ RÃ©ponse HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker.completeJob()                                       â”‚
â”‚  - Parse rÃ©ponse vLLM                                       â”‚
â”‚  - Sauve rÃ©sultat dans result_path                          â”‚
â”‚  - UPDATE status='done', completed_at=now                   â”‚
â”‚  - Trigger fan-in si fragments                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“ Fan-in check
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  fanin.go: checkFanIn()                                     â”‚
â”‚  - COUNT fragments WHERE parent_id=X AND status='done'      â”‚
â”‚  - Si tous done: UPDATE parent status='done'                â”‚
â”‚  - Trigger next workflow stage                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Checklist ImplÃ©mentation

### Phase 1: Nettoyage
- [ ] Supprimer binaires v1 et v2 (`/inference/horos47/bin/horos_gpu_feeder`, `gpu_feeder_v2`)
- [ ] Archiver anciens plans (`/inference/horos47/plans/gpu_feeder*.md`)
- [ ] CrÃ©er rÃ©pertoire v3 (`/inference/horos47/cmd/gpu_feeder_v3/`)

### Phase 2: Fichiers Nouveaux
- [ ] CrÃ©er `worker.go` (worker SQLite)
- [ ] CrÃ©er `http_client.go` (client HTTP vLLM)
- [ ] CrÃ©er `config.go` (configuration v3)
- [ ] CrÃ©er `cmd/gpu_feeder_v3/main.go` (hybride JSON-RPC + worker)

### Phase 3: Fichiers V1 (copier sans modification)
- [ ] Copier `service.go` (v1)
- [ ] Copier `manager.go` (v1)
- [ ] Copier `allocator.go` (v1)
- [ ] Copier `monitor.go` (v1)
- [ ] Copier `health.go` (v1)
- [ ] Copier `types.go` (v1)

### Phase 4: Fichiers V2 (copier sans modification)
- [ ] Copier `db.go` (v2)
- [ ] Copier `fanin.go` (v2)
- [ ] Copier `schema.sql` (v2)

### Phase 5: Modifications V1
- [ ] Ajouter `IsInstanceRunning()` dans `service.go`
- [ ] Ajouter `GetServerURL()` dans `service.go`
- [ ] Ajouter `--max-num-batched-tokens=2048` dans `manager.go`

### Phase 6: Build et Test
- [ ] Compiler: `go build -o bin/gpu_feeder_v3 ./cmd/gpu_feeder_v3`
- [ ] CrÃ©er DB test: `/tmp/gpu_feeder_v3/jobs.db`
- [ ] InsÃ©rer jobs test (rÃ©utiliser script v2)
- [ ] Lancer binaire et vÃ©rifier logs
- [ ] Valider requÃªtes HTTP vers vLLM
- [ ] Valider fan-in sur jobs fragmentÃ©s

---

## ğŸ¯ Validation Finale

### CritÃ¨res de SuccÃ¨s

1. **Serveurs persistants**: Conteneurs vLLM lancÃ©s 1Ã— au dÃ©marrage, pas de restart
2. **HTTP fonctionnel**: RequÃªtes POST vers `localhost:8001` et `8002` rÃ©ussies
3. **Continuous batching**: vLLM batche automatiquement requÃªtes concurrentes
4. **Tracking SQLite**: Jobs transition `pending â†’ processing â†’ done`
5. **Fan-in**: Jobs fragmentÃ©s agrÃ©gÃ©s correctement (parent marquÃ© done)
6. **Orchestration**: StratÃ©gie d'allocation change selon workload
7. **Health check**: Instances unhealthy dÃ©tectÃ©es et relancÃ©es
8. **GPU monitoring**: nvidia-smi toutes les 1s, alertes >80Â°C

### Commandes Test

```bash
# 1. CrÃ©er DB test
cd /tmp
mkdir -p gpu_feeder_v3/stage_vision/pending
sqlite3 gpu_feeder_v3/jobs.db < /inference/horos47/services/gpufeeder/schema.sql

# 2. InsÃ©rer job test
sqlite3 gpu_feeder_v3/jobs.db "
INSERT INTO gpu_jobs (id, payload_sha256, model_type, payload_path, created_at)
VALUES (randomblob(16), 'test123', 'vision', '/tmp/gpu_feeder_v3/stage_vision/pending/test.json', strftime('%s', 'now'));
"

# 3. Lancer GPU Feeder v3
/inference/horos47/bin/gpu_feeder_v3 &

# 4. VÃ©rifier conteneurs actifs
docker ps | grep vllm

# 5. VÃ©rifier jobs processÃ©s
sqlite3 gpu_feeder_v3/jobs.db "SELECT status, COUNT(*) FROM gpu_jobs GROUP BY status;"

# 6. Tester requÃªte HTTP directe
curl -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"model":"/models/qwen2-vl-7b-instruct","messages":[{"role":"user","content":"test"}],"max_tokens":10}'
```

---

## ğŸ“š RÃ©fÃ©rences

- [vLLM Continuous Batching](https://voice.ai/hub/tts/vllm-continuous-batching/) - Architecture continuous batching
- [vLLM Optimization Parameters](https://medium.com/@kaige.yang0110/vllm-throughput-optimization-1-basic-of-vllm-parameters-c39ace00a519) - Tuning `max_num_batched_tokens`
- [vLLM Official Docs](https://docs.vllm.ai/en/stable/configuration/optimization/) - Configuration optimale

---

## ğŸš€ Prochaines Ã‰tapes (Post-V3)

1. **Chunking images**: ImplÃ©menter dÃ©coupage tiles 512x512 avec overlap 120px
2. **Batching adaptatif**: Ajuster `max_num_seqs` selon queue depth
3. **Multi-GPU**: Support tensor parallelism pour gros modÃ¨les
4. **MÃ©triques Prometheus**: Exposer latence, throughput, queue depth
5. **Ray Data**: IntÃ©grer Ray Data pour batching optimisÃ© inter-workers

---

**DurÃ©e estimÃ©e**: 3-4h
**PrioritÃ©**: CRITIQUE (bloque pipeline OCR)
